[1mdiff --git a/hello_app/AP_DQN_web.py b/hello_app/AP_DQN_web.py[m
[1mindex f45b530..ac67132 100644[m
[1m--- a/hello_app/AP_DQN_web.py[m
[1m+++ b/hello_app/AP_DQN_web.py[m
[36m@@ -1,5 +1,14 @@[m
 import numpy as np[m
 import pandas as pd[m
[32m+[m[32mimport gym[m
[32m+[m[32mfrom gym import spaces[m
[32m+[m[32mimport torch[m
[32m+[m[32mimport torch.nn as nn[m
[32m+[m[32mimport torch.optim as optim[m
[32m+[m[32mimport numpy as np[m
[32m+[m[32mimport copy[m
[32m+[m[32mimport time[m
[32m+[m[32mimport random[m
 [m
 def clean_data(file_path):[m
     ohlcv_data = pd.read_csv(f"{file_path}").iloc[::-1].reset_index(drop=True)[m
[36m@@ -14,4 +23,363 @@[m [mdef clean_data(file_path):[m
     ohlcv_data = ohlcv_data.rename(columns=column_names_mapping)[desired_order][m
     ohlcv_data['date'] = pd.to_datetime(ohlcv_data['date'], format='%Y-%m-%d')[m
     ohlcv_data['volume'] = ohlcv_data['volume'].astype(np.int32)[m
[31m-    return ohlcv_data[m
\ No newline at end of file[m
[32m+[m[32m    return ohlcv_data[m
[32m+[m
[32m+[m[32mclass ContinuousOHLCVEnv(gym.Env):[m
[32m+[m[32m    def __init__(self, ohlcv_data: pd.DataFrame, initial_cash: int =10000):[m
[32m+[m[32m        self.ohlcv_raw_data = ohlcv_data[m
[32m+[m[32m        self.initial_cash = initial_cash[m
[32m+[m[32m        self.available_actions = (0,1,2) #DONT THINK ITS BEING USED 0: Hold, 1: Buy, 2: Sell[m
[32m+[m[32m        self.action_space = spaces.Discrete(3)  # 0: Hold, 1: Buy, 2: Sell[m
[32m+[m[32m        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(5,))[m
[32m+[m[32m        self.max_idx = ohlcv_data.shape[0] - 1[m
[32m+[m[32m        self.finish_idx = self.max_idx # New[m
[32m+[m[32m        self.start_idx = 0 # New[m
[32m+[m[32m        self.purchase_history = [] #NEWLY ADDED TO STORE PRICE AT WHICH BOUGHT[m
[32m+[m[32m        self.reset()[m
[32m+[m
[32m+[m[32m    def reset(self):[m
[32m+[m[32m        self.current_step = self.start_idx[m
[32m+[m[32m        self.cash_in_hand = self.initial_cash[m
[32m+[m[32m        self.stock_holding = int(0)[m
[32m+[m[32m        self.step_info = []  # Initialize an empty list to store step information[m
[32m+[m[32m        self.stock_price = self.ohlcv_raw_data[self.current_step][3]  # Assuming closing price for stock price[m
[32m+[m[32m        self.total_portfolio_value = self.cash_in_hand + (self.stock_holding * self.stock_price)[m
[32m+[m[32m        if self.cash_in_hand >= self.stock_price:[m
[32m+[m[32m            self.available_actions = (0, 1)  # Can hold or buy initially[m
[32m+[m[32m        else:[m
[32m+[m[32m            self.available_actions = (0,) ##### Caution: Comma after 0 to make it a tuple maybe delete later[m
[32m+[m[32m        return self.get_observation()[m
[32m+[m[41m    [m
[32m+[m[32m    def step(self, action):[m
[32m+[m[32m        # assert action in self.available_actions, f'Action {action} not in {self.available_actions} '[m
[32m+[m[32m        # if action == 2 and self.cash_in_hand < self.stock_price:[m
[32m+[m[32m        #     action = 0[m
[32m+[m
[32m+[m[32m        prev_valuation = self.total_portfolio_value[m
[32m+[m[32m        # Execute the chosen action[m
[32m+[m[32m        if action == 2:  # SELL[m
[32m+[m[32m            self._sell()[m
[32m+[m[32m        elif action == 0:  # HOLD[m
[32m+[m[32m            pass[m
[32m+[m[32m        elif action == 1:  # BUY[m
[32m+[m[32m            self._buy()[m
[32m+[m[32m        # Update the state after executing the action[m
[32m+[m[32m        self.total_portfolio_value = self.cash_in_hand + (self.stock_holding * self.stock_price)[m
[32m+[m[32m        if action == 0:[m
[32m+[m[32m            if self.total_portfolio_value > prev_valuation:[m
[32m+[m[32m                reward = (self.total_portfolio_value - prev_valuation)*1.25[m
[32m+[m[32m            else:[m
[32m+[m[32m                reward = self.total_portfolio_value - prev_valuation[m[41m   [m
[32m+[m[32m        else:[m
[32m+[m[32m            reward = self.total_portfolio_value - prev_valuation    #########################################changed here cuz was inverted[m
[32m+[m[32m        done = self.current_step >= self.max_idx[m
[32m+[m
[32m+[m[32m        # Record step data AFTER executing the action and updating the state[m
[32m+[m[32m        step_data = {[m
[32m+[m[32m            'Step': self.current_step,[m
[32m+[m[32m            'Portfolio Value': self.total_portfolio_value,[m
[32m+[m[32m            'Cash': self.cash_in_hand,[m
[32m+[m[32m            'Stock Value': self.stock_price * self.stock_holding,[m[41m [m
[32m+[m[32m            'Stock Holdings': self.stock_holding,[m
[32m+[m[32m            'Stock Price': self.stock_price,[m
[32m+[m[32m            'Available Actions': self.available_actions,  # This still reflects the previous state[m
[32m+[m[32m            'Action': action,[m
[32m+[m[32m            "Reward": reward[m
[32m+[m[32m        }[m
[32m+[m[32m        self.step_info.append(step_data)[m
[32m+[m[41m        [m
[32m+[m[32m        # Update available actions for the next step[m
[32m+[m[41m        [m
[32m+[m[32m        if not done:[m
[32m+[m[32m            self.current_step += 1[m
[32m+[m[32m            self.stock_price = self.ohlcv_raw_data[self.current_step][3][m
[32m+[m
[32m+[m[32m#hid        self.update_available_actions()[m
[32m+[m[32m        #print("Chosen action:", action, "Available actions before update:", self.available_actions)[m
[32m+[m[32m        next_observation = self.get_observation()[m
[32m+[m[32m        info = {'available_actions': self.available_actions}  # This will now reflect the updated state[m
[32m+[m[32m        return next_observation, reward, done, info[m
[32m+[m
[32m+[m[32m    def _buy(self):[m
[32m+[m[32m        self.num_stocks_buy = int(np.floor((self.cash_in_hand / self.stock_price) * 1.0))  # Buy up to 90% of stocks possible[m
[32m+[m
[32m+[m[32m        # Check if calculated stocks to buy is zero but cash is enough for at least one stock[m
[32m+[m[32m        if self.num_stocks_buy == 0 and self.cash_in_hand >= self.stock_price:[m
[32m+[m[32m            self.num_stocks_buy = 1  # Buy at least one stock[m
[32m+[m
[32m+[m[32m        elif self.num_stocks_buy > 0:[m
[32m+[m[32m            total_cost = self.num_stocks_buy * self.stock_price[m
[32m+[m[32m            if total_cost <= self.cash_in_hand:  # Check if we have enough cash[m
[32m+[m[32m                self.cash_in_hand -= total_cost[m
[32m+[m[32m                self.stock_holding += self.num_stocks_buy[m
[32m+[m[32m                self.purchase_history.append((self.current_step + 1, self.stock_price, self.num_stocks_buy))[m
[32m+[m[32m                self.available_actions = (0, 2)  # Can hold, buy, or sell[m
[32m+[m
[32m+[m[32m    def _sell(self):[m
[32m+[m[32m        if self.stock_holding > 0:[m
[32m+[m[32m            self.num_stocks_sell = self.stock_holding[m
[32m+[m[32m            total_sale_value = self.stock_holding * self.stock_price[m
[32m+[m[32m            self.cash_in_hand += total_sale_value[m
[32m+[m[32m            self.stock_holding -= self.num_stocks_sell[m
[32m+[m[32m            self.available_actions = (0, 1)  # Can hold or buy[m[41m [m
[32m+[m[32m        else:[m
[32m+[m[32m            # Do nothing if no stocks to sell[m
[32m+[m[32m            pass[m
[32m+[m
[32m+[m
[32m+[m[32m    def get_observation(self): #maybe here different code[m
[32m+[m[32m        return self.ohlcv_raw_data[self.current_step, :].astype(np.float32)[m
[32m+[m
[32m+[m[32m    def get_step_data(self):[m
[32m+[m[32m        return pd.DataFrame(self.step_info)  # Generate a DataFrame from stored[m[41m [m
[32m+[m[32m    #step information[m
[32m+[m
[32m+[m[32m#########DQN###########[m
[32m+[m[41m    [m
[32m+[m[32mclass QNetwork(nn.module):[m
[32m+[m[32m    def __init__(self, input_size, hidden_size, output_size, epsilon: float =1):[m
[32m+[m[32m        super(QNetwork, self).__init__():[m
[32m+[m[32m        self.fc1 = nn.Linear(input_size, hidden_size)[m
[32m+[m[32m        self.fc2 = nn.Linear(hidden_size, hidden_size)[m
[32m+[m[32m        self.fc3 = nn.Linear(hidden_size, output_size)[m
[32m+[m[32m        self.epsilon = epsilon[m
[32m+[m
[32m+[m[32m    def forward(self, x):[m
[32m+[m[32m        x = torch.relu(self.fc1(x))[m
[32m+[m[32m        x = torch.relu(self.fc2(x))[m
[32m+[m[32m        x = self.fc3(x)[m
[32m+[m[32m        return x[m
[32m+[m[32mprint(f"{'epoch':<6} {'epsilon':<8} {'total_step':<11} {'final return':<13} {'elapsed_time':<13} {'final portfolio value':<20}")[m
[32m+[m[32m###TRAIN AND TEST##########[m
[32m+[m
[32m+[m[32m### TRAIN[m[41m [m
[32m+[m
[32m+[m[32mdef train_dqn(env,ohlcv_data_train):[m
[32m+[m[32m    input_size = env.observation_space.shape[0][m
[32m+[m[32m    output_size = env.action_space.n[m
[32m+[m[32m    hidden_size = 100[m
[32m+[m
[32m+[m[32m    Q = QNetwork(input_size=input_size, hidden_size=hidden_size, output_size=output_size)[m
[32m+[m[32m    Q_ast = copy.deepcopy(Q)[m
[32m+[m[32m    optimizer = optim.Adam(Q.parameters(), lr = 0.0001)[m
[32m+[m
[32m+[m
[32m+[m[32m    # Hyperparameters[m
[32m+[m[32m    epoch_num = 10[m
[32m+[m[32m    step_max = len(env.ohlcv_data_train)-1[m
[32m+[m[32m    memory_size = 200[m
[32m+[m[32m    batch_size = 20[m
[32m+[m[32m    train_freq = 50[m
[32m+[m[32m    update_q_freq = 90[m
[32m+[m[32m    gamma = 0.97[m
[32m+[m[32m    show_log_freq = 1[m
[32m+[m[32m    epsilon = 1.0[m
[32m+[m[32m    initial_epsilon = 1.0 #NEW for epsilon decay[m
[32m+[m[32m    #epsilon_decrease = 1e-3 #NEW Not used for this version, since used epsilon function similar to Mike's[m
[32m+[m[32m    epsilon_min = 0.1[m
[32m+[m[32m    epsilon_decay_divisor = 1 #NEW - Smaller divisor means slower decay over epochs[m
[32m+[m[32m    decay_rate = np.log(initial_epsilon/epsilon_min)/(epoch_num/epsilon_decay_divisor)[m
[32m+[m[32m    start_reduce_epsilon = 250  # Start reducing epsilon after this many steps[m
[32m+[m
[32m+[m[32m    memory = [][m
[32m+[m[32m    total_step = 0[m
[32m+[m[32m    total_reward = 0[m
[32m+[m[32m    total_loss = 0[m
[32m+[m[32m    final_returns = [][m
[32m+[m[32m    all_epochs_step_data = [] #NEW - Store all step data for all epochs[m
[32m+[m
[32m+[m[32m    start = time.time()[m
[32m+[m[32m    for epoch in range(epoch_num):[m
[32m+[m[32m        if epsilon > epsilon_min and total_step > start_reduce_epsilon:[m
[32m+[m[32m            epsilon = initial_epsilon * np.exp(-decay_rate * epoch) #NEW - Decay epsilon[m
[32m+[m[32m        pobs = env.reset()[m
[32m+[m[32m        step = 0[m
[32m+[m[32m        done = False[m
[32m+[m[32m        total_reward = 0[m
[32m+[m[32m        total_loss = 0[m
[32m+[m[41m   [m
[32m+[m[32m        while not done and step < step_max:[m
[32m+[m[32m            if len(env.step_info) > 0:[m
[32m+[m[32m                available_actions_choice = env.get_step_data().iloc[-1]["Available Actions"][m
[32m+[m[32m            else:[m
[32m+[m[32m                available_actions_choice = env.available_actions  # Use the initial available actions[m
[32m+[m[32m                # Epsilon-greedy strategy for action selection[m
[32m+[m[32m            if np.random.rand() > epsilon:[m
[32m+[m[32m                # Exploitation: Choose the best action based on Q values[m
[32m+[m[32m                q_values = Q(torch.from_numpy(np.array(pobs, dtype=np.float32)).unsqueeze(0)).detach()[m
[32m+[m[32m                #print(f"Q-values: {q_values}")[m
[32m+[m[32m                # Filter the Q-values based on available actions[m
[32m+[m[32m                filtered_q_values = {action: q_values[0, action].item() for action in available_actions_choice}[m
[32m+[m[32m                #print(f"Filtered Q-values: {filtered_q_values}")[m[41m    [m
[32m+[m[32m                # Sort the filtered Q-values and select the action with the highest Q-value[m
[32m+[m[32m                sorted_actions = sorted(filtered_q_values, key=filtered_q_values.get, reverse=True)[m
[32m+[m[32m                #print(f"Sorted actions: {sorted_actions}")[m
[32m+[m[32m                # Select the best valid action[m
[32m+[m[32m                for action in sorted_actions:[m
[32m+[m[32m                    if action in available_actions_choice:[m
[32m+[m[32m                        pact = action[m
[32m+[m[32m                        break[m
[32m+[m[32m            else:[m
[32m+[m[32m                # Exploration: Randomly choose from available actions[m
[32m+[m[32m                pact = random.choice(list(available_actions_choice))[m
[32m+[m
[32m+[m[32m            # Act[m
[32m+[m[32m            obs, reward, done, _ = env.step(pact)[m
[32m+[m[32m            #print(f"Step {step}: {env.step_info[-1]}")[m[41m [m
[32m+[m
[32m+[m[32m            # Add memory[m
[32m+[m[32m            memory.append((pobs, pact, reward, obs, done)) ### OBS change to state[m
[32m+[m[32m            if len(memory) > memory_size:[m
[32m+[m[32m                memory.pop(0)[m[41m     [m
[32m+[m
[32m+[m[32m            # Train or update Q[m
[32m+[m[32m            if len(memory) == memory_size and total_step % train_freq == 0:[m
[32m+[m[32m                mini_batch = random.sample(memory, batch_size)[m
[32m+[m[32m                b_pobs = torch.from_numpy(np.array([item[0] for item in mini_batch], dtype=np.float32))[m
[32m+[m[32m                b_pact = torch.from_numpy(np.array([item[1] for item in mini_batch], dtype=np.int64))[m
[32m+[m[32m                b_reward = torch.from_numpy(np.array([item[2] for item in mini_batch], dtype=np.float32))[m
[32m+[m[32m                b_obs = torch.from_numpy(np.array([item[3] for item in mini_batch], dtype=np.float32))[m
[32m+[m[32m                b_done = torch.from_numpy(np.array([item[4] for item in mini_batch], dtype=bool))[m
[32m+[m
[32m+[m[32m                q = Q(b_pobs)[m
[32m+[m[32m                maxq = Q_ast(b_obs).detach().max(1)[0][m
[32m+[m[32m                target = q.clone()[m
[32m+[m[32m                for i in range(batch_size):[m
[32m+[m[32m                    target[i, b_pact[i]] = b_reward[i] + gamma * maxq[i] * (not b_done[i])[m
[32m+[m
[32m+[m[32m                # Loss and optimize[m
[32m+[m[32m                optimizer.zero_grad()[m
[32m+[m[32m                loss = nn.functional.mse_loss(q, target)[m
[32m+[m[32m                total_loss += loss.item()[m
[32m+[m[32m                loss.backward()[m
[32m+[m[32m                optimizer.step()[m
[32m+[m
[32m+[m[32m            # Update Q_ast[m
[32m+[m[32m            if total_step % update_q_freq == 0:[m
[32m+[m[32m                Q_ast = copy.deepcopy(Q)[m
[32m+[m[41m                [m
[32m+[m[32m            # Next step[m
[32m+[m[32m            total_reward += reward[m
[32m+[m[32m            pobs = obs[m
[32m+[m[32m            step += 1[m
[32m+[m[32m            total_step += 1[m
[32m+[m
[32m+[m[32m        # Calculate the final portfolio value and return at the end of the epoch[m
[32m+[m[32m        all_epochs_step_data.extend(env.step_info)[m
[32m+[m[32m        final_portfolio_value = env.get_step_data().iloc[-1]["Portfolio Value"][m
[32m+[m[32m        final_return = int((((final_portfolio_value - env.initial_cash) / env.initial_cash) * 100))[m
[32m+[m[32m        final_returns.append(final_return)  # Store the return for this epoch[m
[32m+[m
[32m+[m[32m        # Logging[m
[32m+[m[32m        if (epoch+1) % show_log_freq == 0:[m
[32m+[m[32m            elapsed_time = round((time.time() - start), 2)[m
[32m+[m[32m            print(f"{epoch+1:<6} {round(epsilon, 4):<8} {total_step:<11} {str(final_return) + '%':<14} {elapsed_time:<13} {final_portfolio_value:<20}")[m
[32m+[m[32m            start = time.time()[m
[32m+[m
[32m+[m[32m        # Convert step_data to DataFrame and export[m
[32m+[m[32m        all_epochs_step_data_df = pd.DataFrame(all_epochs_step_data)[m
[32m+[m[32m        all_epochs_step_data_df.to_csv("all_epochs_step_data_action_hold.csv", index=False)[m
[32m+[m
[32m+[m
[32m+[m[32m    # Convert purchase_history to DataFrame and export[m
[32m+[m[32m    purchase_history_df = pd.DataFrame(env.purchase_history, columns=["Step Number",'Price', 'Quantity'])[m
[32m+[m[32m    purchase_history_df.to_csv("purchase_history_action_hold.csv", index=True)[m
[32m+[m
[32m+[m[32m    # Final metric calculation[m
[32m+[m[32m    average_final_return = np.mean(final_returns)[m
[32m+[m[32m    print(f'average_final_return: {average_final_return}'+ "%")[m
[32m+[m[32m    return Q, final_return, final_portfolio_value, average_final_return, final_returns[m
[32m+[m
[32m+[m
[32m+[m[32m#### TEST[m
[32m+[m
[32m+[m[32mdef test_dqn(env, model_path):[m
[32m+[m[32m    model = QNetwork(env.observation_space.shape[0], 100, env.action_space.n)[m
[32m+[m[32m    model.load_state_dict(torch.load(model_path))[m
[32m+[m[32m    model.eval()  # Set the model to evaluation mode[m
[32m+[m[32m    epsilon = 0  # Set a fixed epsilon[m
[32m+[m[32m    total_rewards = [][m
[32m+[m[32m    step_data_list = [][m
[32m+[m[32m    step_max = len(ohlcv_data_test[["open", "high", "low", "close", "volume"]].to_numpy()) #newwwwwwwwwwwwwwwww[m
[32m+[m[32m    final_returns_test = [][m
[32m+[m[32m    initial_cash: int =10000[m
[32m+[m[32m    num_episodes = 100[m
[32m+[m
[32m+[m[32m    for episode in range(num_episodes):[m
[32m+[m[32m        state = env.reset()[m
[32m+[m[32m        episode_reward = 0[m
[32m+[m[32m        done = False[m
[32m+[m[32m        step = 0[m
[32m+[m
[32m+[m[32m        while not done and step < step_max:[m
[32m+[m[32m            # Prepare state for model input[m
[32m+[m[32m            state_tensor = torch.from_numpy(np.array(state, dtype=np.float32)).unsqueeze(0)[m
[32m+[m
[32m+[m[32m            # Get Q-values from the model for the current state[m
[32m+[m[32m            with torch.no_grad():[m
[32m+[m[32m                q_values = model(state_tensor)[m
[32m+[m[32m               # Get the allowed actions in the current state[m
[32m+[m[32m            if len(env.step_info) > 0:[m
[32m+[m[32m                available_actions_choice = env.get_step_data().iloc[-1]["Available Actions"][m
[32m+[m[32m            else:[m
[32m+[m[32m                available_actions_choice = env.available_actions  # Use the initial available actions[m
[32m+[m
[32m+[m[32m            # Choose the action with the highest Q-value among the allowed actions[m
[32m+[m[32m            if np.random.rand() > epsilon:[m
[32m+[m[32m                # Exploitation: Choose the best action based on Q values[m
[32m+[m[32m                filtered_q_values = {action: q_values[0, action].item() for action in available_actions_choice}[m
[32m+[m[32m                print(f"filtered q values: {filtered_q_values}")[m
[32m+[m[32m                sorted_actions = sorted(filtered_q_values, key=filtered_q_values.get, reverse=True)[m
[32m+[m[32m                print(f"sorted actions: {sorted_actions}")[m
[32m+[m[32m                action = next(action for action in sorted_actions if action in available_actions_choice)[m
[32m+[m[32m                print(f"action: {action}")[m
[32m+[m[32m            else:[m
[32m+[m[32m                # Exploration: Randomly choose from available actions[m
[32m+[m[32m                action = random.choice(list(available_actions_choice))[m
[32m+[m
[32m+[m[32m            # Execute the chosen action[m
[32m+[m[32m            next_state, reward, done, _ = env.step(action)[m
[32m+[m[32m            episode_reward += reward[m
[32m+[m[32m            